{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from  PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder, target_size=(48, 32)):\n",
    "    # pomocnicza funkcja do załadowania do zmiennej zdjęć z dysku\n",
    "    # zdjęcia są normalizowane i konwertowane do fat32\n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder)): \n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.resize(target_size)\n",
    "                img = np.array(img) \n",
    "                img = img.astype('float16') / 255.0\n",
    "                images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_segments_from_folder(folder, target_size=(48, 32), num_classes=12):\n",
    "    # pomocnicza funkcja do odczyty posegmentowanych masek\n",
    "    # w pierwotnym założeniu zdjęcia posegmentowane są maskami posiadającymi etykiety w kanale czerwonym\n",
    "    # wartość piksela odpowiada danej klasie\n",
    "    \n",
    "    images = []\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "        img_path = os.path.join(folder, filename)\n",
    "        if os.path.isfile(img_path):\n",
    "            with Image.open(img_path) as img:\n",
    "                img = img.resize(target_size, Image.NEAREST) \n",
    "                img = np.array(img, dtype='uint8')\n",
    "\n",
    "                if img.ndim > 2:\n",
    "                    img = img[..., 0]\n",
    "\n",
    "                img_one_hot = np.zeros((*target_size[::-1], num_classes), dtype='float16') \n",
    "\n",
    "                # encoding one hot\n",
    "                for c in range(num_classes):\n",
    "                    img_one_hot[..., c] = (img == c).astype('float16')\n",
    "\n",
    "                images.append(img_one_hot)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_masks(masks):\n",
    "    if masks.ndim == 4:  # paczka obrazów\n",
    "        n_images, height, width, n_channels = masks.shape\n",
    "        combined_masks = np.zeros((n_images, height, width), dtype=np.uint8)\n",
    "        for i in range(n_images):\n",
    "            for channel in range(n_channels):\n",
    "                combined_masks[i][masks[i, :, :, channel] == 1] = channel + 1\n",
    "    elif masks.ndim == 3:  # pojedynczy obraz\n",
    "        height, width, n_channels = masks.shape\n",
    "        combined_masks = np.zeros((height, width), dtype=np.uint8)\n",
    "        for channel in range(n_channels):\n",
    "            combined_masks[masks[:, :, channel] == 1] = channel + 1\n",
    "    else:\n",
    "        raise ValueError(\"Niewłaściwe wymiary wejściowe. Oczekiwano 3 lub 4 wymiarów.\")\n",
    "\n",
    "    return combined_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images(original_images, processed_images, image_index=0, channel_index=0):\n",
    "    # Złożenie kanałów dla obu obrazów\n",
    "    combined_original = combine_masks(original_images[image_index])\n",
    "    combined_processed = combine_masks(processed_images[image_index])\n",
    "\n",
    "    # Wybór pojedynczych kanałów dla obu obrazów\n",
    "    single_original = original_images[image_index, :, :, channel_index]\n",
    "    single_processed = processed_images[image_index, :, :, channel_index]\n",
    "\n",
    "    # Ustawienie subplots\n",
    "    fig, axs = plt.subplots(1, 4, figsize=(20, 5))  # Dostosuj rozmiar według potrzeb\n",
    "\n",
    "    # Wyświetlenie złożonego pierwszego obrazu\n",
    "    axs[0].imshow(combined_original)\n",
    "    axs[0].set_title('Złożony obraz oryginalny')\n",
    "    axs[0].axis('off')  # Wyłączenie osi\n",
    "\n",
    "    # Wyświetlenie wybranego kanału z pierwszego obrazu\n",
    "    axs[1].imshow(single_original)\n",
    "    axs[1].set_title(f'Kanał {channel_index} oryginalnego')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    # Wyświetlenie wybranego kanału z drugiego obrazu\n",
    "    axs[2].imshow(single_processed)\n",
    "    axs[2].set_title(f'Kanał {channel_index} obrazu przetworzonego')\n",
    "    axs[2].axis('off')\n",
    "\n",
    "    # Wyświetlenie złożonego drugiego obrazu\n",
    "    axs[3].imshow(combined_processed)\n",
    "    axs[3].set_title('Złożony obraz przetworzony')\n",
    "    axs[3].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_weights(weights, num_columns=6):\n",
    "    # funkcja pomocnicza do graficznej reprezentacji wag danej warstwy\n",
    "    #do wywołania z np layer.kernel\n",
    "\n",
    "    num_kernels = weights.shape[3]\n",
    "    \n",
    "    # oblicza liczbę wierszy potrzebnych do wyświetlenia wag\n",
    "    num_rows = num_kernels // num_columns + (num_kernels % num_columns > 0)\n",
    "    \n",
    "    # siatka wykresów\n",
    "    fig, axes = plt.subplots(num_rows, num_columns, figsize=(num_columns, num_rows))\n",
    "    \n",
    "    # normalizacja wag\n",
    "    min_w = np.min(weights)\n",
    "    max_w = np.max(weights)\n",
    "    weights = (weights - min_w) / (max_w - min_w)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < num_kernels:\n",
    "            filter = weights[:, :, :, i]\n",
    "            # jeśli kernel ma więcej niż jeden kanał, wuświetli się w kolorze\n",
    "            if filter.shape[2] == 3:\n",
    "                ax.imshow(filter)\n",
    "            else:\n",
    "                # jeśli to kernel jednowymiarowy, użyj mapy kolorów\n",
    "                ax.imshow(filter.squeeze(), cmap='gray')\n",
    "        \n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "        else:\n",
    "            # ukryj puste subplots\n",
    "            ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient_norm(gradient):\n",
    "    return np.linalg.norm(gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDOptimizer:\n",
    "    def __init__(self, learning_rate=0.01, momentum=0.0, weight_decay=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.velocity = {}\n",
    "        self.weight_decay = weight_decay  # współczynnik regularyzacji L2\n",
    "\n",
    "    def update(self, layer_id, params, grads):\n",
    "        if layer_id not in self.velocity:\n",
    "            self.velocity[layer_id] = np.zeros_like(params)\n",
    "        # zaktualizuj prędkość z uwzględnieniem momentum\n",
    "        self.velocity[layer_id] = self.momentum * self.velocity[layer_id] - self.learning_rate * grads\n",
    "        # dodaj regularyzację L2 do prędkości\n",
    "        self.velocity[layer_id] -= self.learning_rate * self.weight_decay * params\n",
    "        # zaktualizuj parametry\n",
    "        params += self.velocity[layer_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m = {}  # pierwszy moment (średnia ruchoma gradientu)\n",
    "        self.v = {}  # drugi moment (średnia ruchoma kwadratu gradientu)\n",
    "        self.t = 0   # licznik iteracji\n",
    "\n",
    "    def update(self, layer_id, params, grads):\n",
    "        # inicjalizacja m i v, jeśli nie istnieją dla danego id warstwy\n",
    "        if layer_id not in self.m:\n",
    "            self.m[layer_id] = np.zeros_like(params)\n",
    "            self.v[layer_id] = np.zeros_like(params)\n",
    "\n",
    "        # inkrementacja licznika iteracji\n",
    "        self.t += 1\n",
    "\n",
    "        # aktualizacja m i v\n",
    "        self.m[layer_id] = self.beta1 * self.m[layer_id] + (1 - self.beta1) * grads\n",
    "        self.v[layer_id] = self.beta2 * self.v[layer_id] + (1 - self.beta2) * (grads ** 2)\n",
    "\n",
    "        # korekcja odchylenia dla m i v\n",
    "        m_corrected = self.m[layer_id] / (1 - self.beta1 ** self.t)\n",
    "        v_corrected = self.v[layer_id] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "        # aktualizacja parametrów\n",
    "        params -= self.learning_rate * m_corrected / (np.sqrt(v_corrected) + self.epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @staticmethod\n",
    "    def binary_cross_entropy(predicted, true):\n",
    "        # mały epsilon dla dzielenia przez 0\n",
    "        epsilon = 1e-15\n",
    "\n",
    "        # ograniczenie przewidywanych wartości do zakresu [epsilon, 1 - epsilon] aby zapobiec błędom numerycznym podczas logarytmowania\n",
    "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Obliczanie entropii krzyżowej między prawdziwymi a przewidywanymi etykietami dla klasyfikacji binarnej\n",
    "        return -np.mean(true * np.log(predicted) + (1 - true) * np.log(1 - predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableSigmoid:\n",
    "    def forward(self, x):\n",
    "        positive_mask = (x >= 0)\n",
    "        negative_mask = (x < 0)\n",
    "        z = np.zeros_like(x)\n",
    "        z[positive_mask] = 1 / (1 + np.exp(-x[positive_mask]))\n",
    "        z[negative_mask] = np.exp(x[negative_mask]) / (1 + np.exp(x[negative_mask]))\n",
    "    \n",
    "        self.output = z\n",
    "        return z\n",
    "    def backward(self, dout):\n",
    "        dz = self.output * (1 - self.output) * dout\n",
    "        return dz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StableSoftmax:\n",
    "    def forward(self, x):\n",
    "        # odjęcie maksymalnej wartość w każdym kanale dla stabilności numerycznej\n",
    "        shift_x = x - np.max(x, axis=-1, keepdims=True)\n",
    "        exps = np.exp(shift_x)\n",
    "        self.output = exps / np.sum(exps, axis=-1, keepdims=True)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.zeros_like(dout)\n",
    "\n",
    "        # uproszczona metoda bez obliczania macierzy jacobiego- ograniczone zasoby obliczeniowe\n",
    "        for i in range(dout.shape[0]):\n",
    "            for h in range(dout.shape[1]):\n",
    "                for w in range(dout.shape[2]):\n",
    "                    softmax_output = self.output[i, h, w]\n",
    "                    sum_dout_softmax = np.sum(dout[i, h, w] * softmax_output)\n",
    "                    dx[i, h, w] = softmax_output * (dout[i, h, w] - sum_dout_softmax)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer:\n",
    "    def forward(self, input):\n",
    "        self.input = input \n",
    "        return np.maximum(0, input)\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dinput = dout.copy()\n",
    "        dinput[self.input <= 0] = 0\n",
    "        return dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLULayer:\n",
    "    def __init__(self, alpha=0.01):\n",
    "        # alpha- małe współczynniki do \"przeciekania\" ujemnych gradientów\n",
    "        self.alpha = alpha \n",
    "\n",
    "    def forward(self, input):\n",
    "        # LeakyReLU = max(x, mała stała * x),\n",
    "        self.input = input\n",
    "        return np.where(input > 0, input, self.alpha * input)\n",
    "\n",
    "    def backward(self, output_gradient):\n",
    "        if self.input.shape != output_gradient.shape:\n",
    "            raise ValueError(f\"Niepoprawny kształt gradientu wyjściowego dla LeakyReLU: oczekiwano {self.input.shape}, otrzymano {output_gradient.shape}\")\n",
    "        # gradient to 1 dla x > 0 i alpha dla x <= 0\n",
    "        input_gradient = np.where(self.input > 0, 1, self.alpha)\n",
    "        return output_gradient * input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolingLayer:\n",
    "    def __init__(self, pool_size=2):\n",
    "        self.pool_size = pool_size\n",
    "        self.input = None\n",
    "        self.max_indices = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # dane wejściowe są zapisywane, następnie przeprowadzany jest forward pass\n",
    "        self.input = input\n",
    "        B, H, W, C = input.shape \n",
    "\n",
    "        # tablica dla indeksów maksymalnych wartości i wyniku poolingu, jest wykorzystywana do bacward pass\n",
    "        self.max_indices = np.zeros((B, H // self.pool_size, W // self.pool_size, C), dtype=int)\n",
    "        output = np.zeros((B, H // self.pool_size, W // self.pool_size, C))\n",
    "\n",
    "        # iteracja po liczbie danych, kernelach, wyokości i szerokości mapy\n",
    "        for batch in range(B):\n",
    "            for kernel in range(C):\n",
    "                for height in range(0, H, self.pool_size):\n",
    "                    for width in range(0, W, self.pool_size):\n",
    "                        # wycięcie fragmentu danych wejściowych\n",
    "                        slice = input[batch, height:height+self.pool_size, width:width+self.pool_size, kernel]\n",
    "                        # zapisanie indeksu maksymalnej wartości w wycinku\n",
    "                        self.max_indices[batch, height // self.pool_size, width // self.pool_size, kernel] = np.argmax(slice)\n",
    "                        # zapis maksymalnej wartości\n",
    "                        output[batch, height // self.pool_size, width // self.pool_size, kernel] = np.max(slice)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        B, H, W, C = self.input.shape\n",
    "        dinput = np.zeros_like(self.input)\n",
    "\n",
    "        for batch in range(B):\n",
    "            for depth in range(C):\n",
    "                for height in range(dout.shape[1]):\n",
    "                    for width in range(dout.shape[2]):\n",
    "                        # wyznaczenie indeksów w oryginalnym obrazie\n",
    "                        index = self.max_indices[batch, height, width, depth]\n",
    "                        h_idx = height * self.pool_size + index // self.pool_size\n",
    "                        w_idx = width * self.pool_size + index % self.pool_size\n",
    "                        # przypisanie gradientu do odpowiedniego miejsca\n",
    "                        dinput[batch, h_idx, w_idx, depth] = dout[batch, height, width, depth]\n",
    "\n",
    "        return dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSamplingLayer:\n",
    "    def __init__(self, scale=2):\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input \n",
    "        return np.repeat(np.repeat(input, self.scale, axis=1), self.scale, axis=2)\n",
    "\n",
    "    # def backward(self, dout):\n",
    "    #     B, H, W, C = self.input.shape\n",
    "    #     dout_downsampled = dout[:, ::self.scale, ::self.scale, :]\n",
    "    #     return dout_downsampled\n",
    "    def backward(self, dout):\n",
    "        B, H, W, C = self.input.shape\n",
    "        dout_upsampled = np.zeros_like(self.input)\n",
    "        \n",
    "        for b in range(B):\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    for c in range(C):\n",
    "                        i = h * self.scale\n",
    "                        j = w * self.scale\n",
    "                        # Gradient jest rozdzielany równomiernie pomiędzy wszystkie elementy, które zostały zduplikowane w forward\n",
    "                        dout_upsampled[b, h, w, c] = np.sum(dout[b, i:i+self.scale, j:j+self.scale, c]) / (self.scale**2)\n",
    "        \n",
    "        return dout_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormLayer:\n",
    "    #[Online]. Available: https://proceedings.mlr.press/v37/ioffe15.html.\n",
    "    #[Online]. Available: https://github.com/renan-cunha/BatchNormalization.\n",
    "    def __init__(self, num_features, name=None):\n",
    "        self.gamma = np.ones((1, num_features))\n",
    "        self.beta = np.zeros((1, num_features))\n",
    "        self.moving_mean = np.zeros((1, num_features))\n",
    "        self.moving_variance = np.ones((1, num_features))  # inicjalizacja na 1 zapobiega dzieleniu przez 0\n",
    "\n",
    "        self.name = name\n",
    "        self.epsilon = 1e-3\n",
    "        self.training = False\n",
    "\n",
    "    def forward(self, X, momentum=0.9):\n",
    "        B, H, W, C = X.shape\n",
    "        self.input_reshaped = X.reshape(-1, C)\n",
    "\n",
    "        if self.training:\n",
    "            self.batch_mean = np.mean(self.input_reshaped, axis=0, keepdims=True)\n",
    "            self.batch_variance = np.var(self.input_reshaped, axis=0, keepdims=True)\n",
    "\n",
    "            self.moving_mean = momentum * self.moving_mean + (1 - momentum) * self.batch_mean\n",
    "            self.moving_variance = momentum * self.moving_variance + (1 - momentum) * self.batch_variance\n",
    "\n",
    "            self.X_normalized = (self.input_reshaped - self.batch_mean) / np.sqrt(self.batch_variance + self.epsilon)\n",
    "        else:\n",
    "            self.X_normalized = (self.input_reshaped - self.moving_mean) / np.sqrt(self.moving_variance + self.epsilon)\n",
    "\n",
    "        self.output_reshaped = self.gamma * self.X_normalized + self.beta\n",
    "        return self.output_reshaped.reshape(B, H, W, C)\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        B, H, W, C = grad_output.shape\n",
    "        grad_output_reshaped = grad_output.reshape(-1, C)\n",
    "\n",
    "        self.dbeta = np.sum(grad_output_reshaped, axis=0)\n",
    "        self.dgamma = np.sum(self.X_normalized * grad_output_reshaped, axis=0)\n",
    "\n",
    "        dX_normalized = grad_output_reshaped * self.gamma\n",
    "        dX_mu1 = dX_normalized * 1 / np.sqrt(self.batch_variance + self.epsilon)\n",
    "        dX_mu2 = np.sum(dX_normalized * -1 / np.sqrt(self.batch_variance + self.epsilon), axis=0)\n",
    "        dvar = np.sum(dX_normalized * -0.5 * (self.input_reshaped - self.batch_mean) / (self.batch_variance + self.epsilon)**(3/2), axis=0)\n",
    "\n",
    "        dX = (dX_mu1 + (2 / B) * (self.input_reshaped - self.batch_mean) * dvar + dX_mu2 / B).reshape(B, H, W, C)\n",
    "        \n",
    "        return dX\n",
    "\n",
    "    def update_params(self, learning_rate):\n",
    "        self.gamma -= learning_rate * self.dgamma\n",
    "        self.beta -= learning_rate * self.dbeta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, num_kernels, kernel_size, kernel_depth, stride=1, padding=0, name=None):\n",
    "        # init obiektu warstwy konwolucyjnej\n",
    "        # parametry : liczba kerneli, rozmiar kerneli (kwadratowy size x size), głębokość kerneli, stride i padding z wartościami domyślnymi\n",
    "        # dkernels przechowuje gradienty dla kerneli\n",
    "        # inicjalizacja wag \n",
    "        \n",
    "        self.num_kernels = num_kernels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.kernel_depth = kernel_depth\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.name = name\n",
    "        # inicjalizacja wag z rozkładu normalnego\n",
    "        #self.kernels = np.random.standard_normal((num_kernels, kernel_size, kernel_size, kernel_depth))\n",
    "        # inicjalizacja He\n",
    "        limit = np.sqrt(6 / (kernel_depth + num_kernels))\n",
    "        self.kernels = np.random.uniform(-limit, limit, (num_kernels, kernel_size, kernel_size, kernel_depth))\n",
    "        # [Online]. Available: https://medium.com/@shauryagoel/kaiming-he-initialization-a8d9ed0b5899\n",
    "        # [Online]. Available: https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks.\n",
    "\n",
    "        self.dkernels = np.zeros_like(self.kernels)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # metoda w przód warstwy konwolucyjnej\n",
    "        # pobiera obraz jako argument, zwraca przetworozny obraz\n",
    "        # pozostałe dane są właściwością obiektu\n",
    "        \n",
    "        self.input = input \n",
    "        stride = self.stride\n",
    "        padding = self.padding\n",
    "\n",
    "        # padding mapy wejściowej\n",
    "        input_padded = np.pad(input, [(0, 0), \n",
    "                                      (padding, padding),\n",
    "                                      (padding, padding), \n",
    "                                      (0, 0)], \n",
    "                                    mode='edge')\n",
    "\n",
    "        # rozmiar mapy wejściowej\n",
    "        batch_size, input_height, input_width, input_channels = input_padded.shape\n",
    "\n",
    "        # obsługa błędu liczby kanałów\n",
    "        if input_channels != self.kernels.shape[3]:\n",
    "            raise ValueError(\"The number of input channels must match the number of kernel channels.\")\n",
    "\n",
    "        # rozmiar mapy wyjściowej\n",
    "        output_height = ((input_height - self.kernel_size) // stride) + 1\n",
    "        output_width = ((input_width - self.kernel_size) // stride) + 1\n",
    "\n",
    "        # init mapy wyjściowej\n",
    "        output = np.zeros((batch_size, output_height, output_width, self.num_kernels))\n",
    "\n",
    "\n",
    "        # # wersja oryginalna\n",
    "        # #___________________________________________________________________________________________________________________________________\n",
    "        # # pętla po przykąłdach w batchu\n",
    "        # for batch in range(batch_size):\n",
    "        #     # pętla po kernelach\n",
    "        #     for kernel in range(self.num_kernels):\n",
    "        #         # pętla pionowa\n",
    "        #         for height in range(0, output_height):\n",
    "        #             # pętla pozioma\n",
    "        #             for width in range(0, output_width):\n",
    "        #                 # określenie rogów wycinka\n",
    "        #                 vertical_start = height * stride\n",
    "        #                 vertical_end = vertical_start + self.kernel_size\n",
    "        #                 horizontal_start = width * stride\n",
    "        #                 horizontal_end = horizontal_start + self.kernel_size\n",
    "\n",
    "        #                 # wycięzcie fragmentu macierzy\n",
    "        #                 current_slice = input_padded[batch, vertical_start:vertical_end, horizontal_start:horizontal_end, :]\n",
    "        #                 # operacja konwolucji\n",
    "        #                 output[batch, height, width, kernel] = np.sum(current_slice * self.kernels[kernel, :, :, :])\n",
    "        # #___________________________________________________________________________________________________________________________________\n",
    "\n",
    "        # # Wersja 2, szybsza\n",
    "        #___________________________________________________________________________________________________________________________________\n",
    "        # reshape kerneli\n",
    "        kernels_reshaped = self.kernels.reshape(self.num_kernels, -1).T\n",
    "        # pętle po wysokości i szerokości mapy\n",
    "        for i in range(output_height):\n",
    "            for j in range(output_width):\n",
    "                # wycinanie fragmentów macierzy\n",
    "                slice = input_padded[:, i*stride:i*stride+self.kernel_size, j*stride:j*stride+self.kernel_size, :]\n",
    "                slice_reshaped = slice.reshape(batch_size, -1)\n",
    "\n",
    "                # konwolucja wektorowa\n",
    "                output[:, i, j, :] = np.dot(slice_reshaped, kernels_reshaped).reshape(-1, self.num_kernels)\n",
    "        #___________________________________________________________________________________________________________________________________\n",
    "\n",
    "        return output\n",
    "\n",
    "    def backward(self, dout):\n",
    "        # dinput gradient względem wejścia - pochodna błędu względem wejścia\n",
    "        # dout gradient względem wyjścia - pochodna błędu względem wyjścia\n",
    "        \n",
    "        stride = self.stride\n",
    "        padding = self.padding\n",
    "        batch_size, dout_height, dout_width, num_kernels = dout.shape\n",
    "        dinput = np.zeros_like(self.input)\n",
    "        padded_input = np.pad(self.input, \n",
    "                            [(0, 0), \n",
    "                            (padding, padding), \n",
    "                            (padding, padding), \n",
    "                            (0, 0)], \n",
    "                            mode='edge')\n",
    "        padded_dinput = np.pad(dinput, \n",
    "                            [(0, 0), \n",
    "                            (padding, padding), \n",
    "                            (padding, padding), \n",
    "                            (0, 0)], \n",
    "                            mode='edge')\n",
    "        \n",
    "        # obliczenie gradientu dla wag\n",
    "        for kernel in range(num_kernels):\n",
    "            for batch in range(batch_size):\n",
    "                for height in range(dout_height):\n",
    "                    for width in range(dout_width):\n",
    "                        vertical_start = height * stride\n",
    "                        vertical_end = vertical_start + self.kernel_size\n",
    "                        horizontal_start = width * stride\n",
    "                        horizontal_end = horizontal_start + self.kernel_size\n",
    "                        padded_input_slice = padded_input[batch, vertical_start:vertical_end, horizontal_start:horizontal_end, :]\n",
    "                        # wkład w gradient\n",
    "                        self.dkernels[kernel] += dout[batch, height, width, kernel] * padded_input_slice\n",
    "\n",
    "        # normalizacja gradientów przez rozmiar batcha\n",
    "        self.dkernels /= batch_size\n",
    "\n",
    "        # obliczenie gradientu dla wejść\n",
    "        for batch in range(batch_size):\n",
    "            for height in range(dout_height):\n",
    "                for width in range(dout_width):\n",
    "                    vertical_start = height * stride\n",
    "                    vertical_end = vertical_start + self.kernel_size\n",
    "                    horizontal_start = width * stride\n",
    "                    hori_end = horizontal_start + self.kernel_size\n",
    "\n",
    "                    # dla wszystkich kerneli\n",
    "                    for kernel in range(num_kernels):\n",
    "                        # w dinput akumulują się gradieint wyjścia * wagi\n",
    "                        padded_dinput[batch, vertical_start:vertical_end, horizontal_start:hori_end, :] += self.kernels[kernel] * dout[batch, height, width, kernel]\n",
    "\n",
    "        # usunięcie paddingu\n",
    "        if padding != 0:\n",
    "            dinput = padded_dinput[:, padding:-padding, padding:-padding, :]\n",
    "        else: dinput = padded_dinput\n",
    "\n",
    "        return dinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoencoder:\n",
    "    def __init__(self, learning_rate=0.001, momentum=0.0, weight_decay=0.0, clip_value=1.0):\n",
    "        # architektura wzorozwana na sieci U-Net\n",
    "        #[Online]. Available: https://www.analyticsvidhya.com/blog/2023/08/unet-architecture-mastering-image-segmentation/?fbclid=IwAR2cYayYBhSfBZTxNCfOMrRElCvhU_M1JYhP91DmFYdttUEe3zB3Bro\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.clip_value = clip_value  # maksymalna norma gradientu\n",
    "        # instancja sgd na init klasy\n",
    "        self.optimizer = SGDOptimizer(learning_rate=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "        # wybór optimizera Adam\n",
    "        # self.optimizer = AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999)\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = ConvLayer(num_kernels=16, kernel_size=3, kernel_depth=3, stride=1, padding=1, name='conv1')\n",
    "        self.bn1 = BatchNormLayer(num_features=16, name='bn1')\n",
    "        self.relu1 = LeakyReLULayer()\n",
    "        self.pool1 = PoolingLayer(2)\n",
    "\n",
    "        self.conv2 = ConvLayer(num_kernels=32, kernel_size=3, kernel_depth=16, stride=1, padding=1, name='conv2')\n",
    "        self.bn2 = BatchNormLayer(num_features=32, name='bn2')\n",
    "        self.relu2 = LeakyReLULayer()\n",
    "        self.pool2 = PoolingLayer(2)\n",
    "        \n",
    "        # Mid\n",
    "        self.convmid = ConvLayer(num_kernels= 64, kernel_size=3, kernel_depth=32, stride=1, padding=1, name='convmid')\n",
    "        self.bnmid = BatchNormLayer(num_features=64, name='bnmid')\n",
    "        self.relumid = LeakyReLULayer()\n",
    "        \n",
    "        # Decoder\n",
    "        self.upsample1 = UpSamplingLayer(2)\n",
    "        self.conv3 = ConvLayer(num_kernels=32, kernel_size=3, kernel_depth=64, stride=1, padding=1, name='conv3')\n",
    "        self.bn3 = BatchNormLayer(num_features=32, name='bn3')\n",
    "        self.relu3 = LeakyReLULayer()\n",
    "        \n",
    "        self.upsample2 = UpSamplingLayer(2)\n",
    "        self.conv4 = ConvLayer(num_kernels=16, kernel_size=3, kernel_depth=32, stride=1, padding=1, name='conv4')\n",
    "        self.bn4 = BatchNormLayer(num_features=16, name='bn4')\n",
    "        self.relu4 = LeakyReLULayer()\n",
    "        \n",
    "        self.convend = ConvLayer(num_kernels=12, kernel_size=1, kernel_depth=32, stride=1, padding=1, name='convend')\n",
    "        self.activation = StableSoftmax()\n",
    "\n",
    "    def forward(self, x_input):\n",
    "        # Encoder\n",
    "        x = self.conv1.forward(x_input)\n",
    "        x = self.bn1.forward(x) \n",
    "        x = self.relu1.forward(x)\n",
    "        skipcon1 = x.copy()\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.bn2.forward(x) \n",
    "        x = self.relu2.forward(x)\n",
    "        skipcon2 = x.copy()\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        # Mid\n",
    "        x = self.convmid.forward(x)\n",
    "        x = self.bnmid.forward(x) \n",
    "        x = self.relumid.forward(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.upsample1.forward(x)\n",
    "        x = self.conv3.forward(x)\n",
    "        x = self.bn3.forward(x)\n",
    "        x = self.relu3.forward(x)\n",
    "        \n",
    "        # + drugi skip connection\n",
    "        x = x + .2 * skipcon2  \n",
    "        \n",
    "        x = self.upsample2.forward(x)\n",
    "        x = self.conv4.forward(x)\n",
    "        x = self.bn4.forward(x)\n",
    "        x = self.relu4.forward(x)\n",
    "        \n",
    "        # + pierwszy skip connection\n",
    "        x = x + .2 * skipcon1\n",
    "        \n",
    "        x = self.convend.forward(x)\n",
    "        decoded = self.activation.forward(x)\n",
    "        \n",
    "        return decoded\n",
    "\n",
    "    def backward(self, Y_pred, Y_batch):\n",
    "        # obliczenia na podstawie różnicy od wartości treningowej\n",
    "        dout = self.activation.backward(Y_pred - Y_batch)\n",
    "        dout = self.convend.backward(dout)\n",
    "\n",
    "        dout = self.relu4.backward(dout)\n",
    "        dout = self.bn4.backward(dout)\n",
    "        dout = self.conv4.backward(dout)\n",
    "        \n",
    "        dout = self.upsample2.backward(dout)\n",
    "        dout = self.relu3.backward(dout)\n",
    "        dout = self.bn3.backward(dout) \n",
    "        dout = self.conv3.backward(dout)\n",
    "        \n",
    "        dout = self.upsample1.backward(dout)\n",
    "        dout = self.relumid.backward(dout)\n",
    "        dout = self.bnmid.backward(dout)\n",
    "        dout = self.convmid.backward(dout)\n",
    "\n",
    "        dout = self.pool2.backward(dout)\n",
    "        dout = self.relu2.backward(dout)\n",
    "        dout = self.bn2.backward(dout)\n",
    "        dout = self.conv2.backward(dout)\n",
    "        \n",
    "        dout = self.pool1.backward(dout)\n",
    "        dout = self.relu1.backward(dout)\n",
    "        dout = self.bn1.backward(dout)\n",
    "        final_dout = self.conv1.backward(dout)\n",
    "\n",
    "        return final_dout\n",
    "    \n",
    "    def update_weights(self):\n",
    "        # clipping gradientów przed aktualizacją wag\n",
    "        if self.clip_value != 0:    # można wyąłczyć ustawiając 0\n",
    "            for layer in [self.conv1, self.conv2, self.convmid, self.conv3, self.conv4, self.convend]:\n",
    "                # norma gradientów\n",
    "                gradient_norm = np.linalg.norm(layer.dkernels)\n",
    "                \n",
    "                # jeśli przekracza próg, jest przeskalowana\n",
    "                if gradient_norm > self.clip_value:\n",
    "                    layer.dkernels *= self.clip_value / gradient_norm\n",
    "\n",
    "        # aktualizacja wag optymizerem\n",
    "        for i, layer in enumerate([self.conv1, self.conv2, self.convmid, self.conv3, self.conv4, self.convend]):\n",
    "            self.optimizer.update(i, layer.kernels, layer.dkernels, )\n",
    "\n",
    "        # aktualizacja BN\n",
    "        for bn_layer in [self.bn1, self.bn2, self.bnmid, self.bn3, self.bn4]:\n",
    "            bn_layer.update_params(self.learning_rate)\n",
    "        \n",
    "    def set_training_mode(self, mode):\n",
    "        # funkcja załączająca/ wyłączająca warstwy BN (True/Flase)\n",
    "        self.bn1.training = mode\n",
    "        self.bn2.training = mode\n",
    "        self.bnmid.training = mode\n",
    "        self.bn3.training = mode\n",
    "        self.bn4.training = mode\n",
    "    \n",
    "    def train(self, X_train, Y_train, X_val, Y_val, epochs, batch_size):\n",
    "        # listy pomocnicze\n",
    "        losses = []\n",
    "        val_losses = []\n",
    "        gradient_norms = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # zerowanie loss i lista pomocnicza\n",
    "            epoch_loss = 0\n",
    "            gradient_norms_per_epoch = []\n",
    "\n",
    "            # obliczenie liczby batchy z liczby zdjęć treningowych\n",
    "            num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
    "\n",
    "            # iteracja po batchach\n",
    "            for batch_idx in range(num_batches):\n",
    "                # przeliczenie indeksów batcha\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = min(start_idx + batch_size, X_train.shape[0])\n",
    "                X_batch = X_train[start_idx:end_idx]\n",
    "                Y_batch = Y_train[start_idx:end_idx]\n",
    "\n",
    "                # forward pass\n",
    "                # Y_pred jest obrazem wyjściowym (12 kanałów)\n",
    "                Y_pred = self.forward(X_batch)\n",
    "\n",
    "                # obliczenie loss funkcją straty\n",
    "                # [Online]. Available: https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a.\n",
    "                batch_loss = LossFunction.binary_cross_entropy(Y_pred, Y_batch)\n",
    "                epoch_loss += batch_loss\n",
    "                \n",
    "                # zerowanie gradientów\n",
    "                for layer in [self.conv1, self.conv2, self.convmid, self.conv3, self.conv4, self.convend, self.bn1, self.bn2, self.bnmid, self.bn3, self.bn4]:\n",
    "                    if hasattr(layer, 'dkernels'):\n",
    "                        layer.dkernels = np.zeros_like(layer.dkernels)\n",
    "                    if hasattr(layer, 'dgamma'):\n",
    "                        layer.dgamma = np.zeros_like(layer.gamma)\n",
    "                    if hasattr(layer, 'dbeta'):\n",
    "                        layer.dbeta = np.zeros_like(layer.beta)\n",
    "\n",
    "                # backward pass\n",
    "                self.backward(Y_pred, Y_batch)\n",
    "                # aktualizacja wag\n",
    "                self.update_weights()\n",
    "\n",
    "                # gradient norms z batcha\n",
    "                batch_gradient_norms = []\n",
    "                for layer in [self.conv1, self.conv2, self.convmid, self.conv3, self.conv4, self.convend]:\n",
    "                    norm = calculate_gradient_norm(layer.dkernels)\n",
    "                    batch_gradient_norms.append(norm)\n",
    "                gradient_norms_per_epoch.append(batch_gradient_norms)\n",
    "\n",
    "            # średni gradient norm dla epoki\n",
    "            average_gradient_norms = np.mean(gradient_norms_per_epoch, axis=0)\n",
    "            gradient_norms.append(average_gradient_norms)\n",
    "\n",
    "            # zapis losses i gradient norms\n",
    "            epoch_average_loss = epoch_loss / num_batches\n",
    "            losses.append(epoch_average_loss)\n",
    "            \n",
    "            # walidacja\n",
    "            Y_pred_val = self.forward(X_val)\n",
    "            val_loss = LossFunction.binary_cross_entropy(Y_pred_val, Y_val)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            # Zapisywanie i wyświetlanie wyników\n",
    "            print(f'Epoch {epoch + 1}/{epochs}, Loss: {epoch_average_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "        return losses, gradient_norms\n",
    "\n",
    "    def export_weights(self, file_path):\n",
    "        weights = {}\n",
    "        for layer in [self.conv1, self.conv2, self.convmid, self.conv3, self.conv4, self.convend, self.bn1, self.bn2, self.bnmid, self.bn3, self.bn4]:\n",
    "            layer_name = layer.name\n",
    "\n",
    "            if hasattr(layer, 'kernels'):\n",
    "                weights[layer_name + '_kernels'] = {\n",
    "                    'shape': layer.kernels.shape,\n",
    "                    'values': layer.kernels.tolist()\n",
    "                }\n",
    "\n",
    "            if isinstance(layer, BatchNormLayer):\n",
    "                weights[layer_name + '_gamma'] = layer.gamma.tolist()\n",
    "                weights[layer_name + '_beta'] = layer.beta.tolist()\n",
    "                weights[layer_name + '_moving_mean'] = layer.moving_mean.tolist()\n",
    "                weights[layer_name + '_moving_variance'] = layer.moving_variance.tolist()\n",
    "\n",
    "        with open(file_path, 'w') as file:\n",
    "            json.dump(weights, file)\n",
    "\n",
    "    def import_weights(self, file_path):\n",
    "        with open(file_path, 'r') as file:\n",
    "            weights = json.load(file)\n",
    "            \n",
    "        for layer in [self.conv1, self.conv2, self.convmid, self.conv3, self.conv4, self.convend, self.bn1, self.bn2, self.bnmid, self.bn3, self.bn4]:\n",
    "            layer_name = layer.name\n",
    "\n",
    "            if layer_name + '_kernels' in weights:\n",
    "                kernel_data = weights[layer_name + '_kernels']\n",
    "                layer.kernels = np.array(kernel_data['values']).reshape(kernel_data['shape'])\n",
    "\n",
    "            if isinstance(layer, BatchNormLayer):\n",
    "                if layer_name + '_gamma' in weights:\n",
    "                    layer.gamma = np.array(weights[layer_name + '_gamma'])\n",
    "                if layer_name + '_beta' in weights:\n",
    "                    layer.beta = np.array(weights[layer_name + '_beta'])\n",
    "                if layer_name + '_moving_mean' in weights:\n",
    "                    layer.moving_mean = np.array(weights[layer_name + '_moving_mean'])\n",
    "                if layer_name + '_moving_variance' in weights:\n",
    "                    layer.moving_variance = np.array(weights[layer_name + '_moving_variance'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________________\n",
    "ŁADOWANIE ZDJĘĆ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ścieżki do folderów\n",
    "\n",
    "folder_original = 'images\\\\train\\\\img'\n",
    "folder_segmented = 'images\\\\train\\\\seg'\n",
    "folder_val_img = 'images\\\\validation\\\\img'\n",
    "folder_val_seg = 'images\\\\validation\\\\seg'\n",
    "folder_test_img = 'images\\\\test\\\\img'\n",
    "folder_test_seg = 'images\\\\test\\\\seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZAŁADOWANIE OBRAZÓW\n",
    "try:\n",
    "    original_images = load_images_from_folder(folder_original)\n",
    "    segmented_images = load_segments_from_folder(folder_segmented)\n",
    "    val_images = load_images_from_folder(folder_val_img)\n",
    "    val_segment = load_segments_from_folder(folder_val_seg)\n",
    "    test_images = load_images_from_folder(folder_test_img)\n",
    "    test_segment = load_segments_from_folder(folder_test_seg)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________________________________________________________________________________________________________________________\n",
    "STEROWANIE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "    # INIT MODELU\n",
    "X_SET = original_images\n",
    "Y_SET = segmented_images\n",
    "X_VAL = val_images\n",
    "Y_VAL = val_segment\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 3\n",
    "LEARNING_RATE = .0001\n",
    "WEIGHT_DECAY = .00001\n",
    "SGD_MOMENTUM = .9\n",
    "CLIP_VALUE = 0\n",
    "autoenkoder = CNNAutoencoder(learning_rate=LEARNING_RATE, momentum=SGD_MOMENTUM, weight_decay=WEIGHT_DECAY, clip_value=CLIP_VALUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # TRENING MODELU\n",
    "autoenkoder.set_training_mode(True)\n",
    "loss, history_weights = autoenkoder.train(X_SET, Y_SET, X_VAL, Y_VAL, epochs= EPOCHS, batch_size= BATCH_SIZE)\n",
    "autoenkoder.set_training_mode(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   EXPORT MODELU          \n",
    "model_path = 'weights'\n",
    "autoenkoder.export_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT MODELU\n",
    "model_path = 'weights'\n",
    "autoenkoder.import_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREZENTACJA WYNIKÓW:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # FORWARD PASS OBRAZÓW           \n",
    "autoenkoder.set_training_mode(False)\n",
    "convoled = autoenkoder.forward(original_images[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''     Opis etykiet (kanałów obrazu):\n",
    "0   TŁO\n",
    "1   BUDYNKI\n",
    "2   OGRODZENIA / PŁOTY / BARIERKI\n",
    "3   ELEMENTY BUDYNKÓW\n",
    "4   PIESI\n",
    "5   SŁUPY / LATARNIE\n",
    "6   ZNAKI POZIOME\n",
    "7   JEZDNIA\n",
    "8   CHODNIK\n",
    "9   ROŚLINNOŚĆ\n",
    "10  POJAZDY\n",
    "11  SYGNALIZACJA ŚWIETLNA\n",
    "'''\n",
    "\n",
    "# idex obrazu w zbiorze\n",
    "image_index = 0\n",
    "# idex kanału do wyświetlenia\n",
    "image_channel = 7\n",
    "display_images(segmented_images, convoled, image_index=image_index, channel_index=image_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_weights(autoenkoder.relumid.forward(autoenkoder.convmid.kernels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
